{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assessment -1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION"
      ],
      "metadata": {
        "id": "zzu5soDblX45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD TOKENIZATION"
      ],
      "metadata": {
        "id": "8JwOJdV_mO3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am Seetha\"\n",
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuYSLB4vnhKy",
        "outputId": "96ed8614-113c-4a0c-dc88-748261541ffb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'Seetha']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak\"\n",
        "text.split(' . ') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62CIRFlUldNv",
        "outputId": "5b0e13ae-1751-44db-8ae0-555db05f13c2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION USING REGULAR EXPRESSION (re)"
      ],
      "metadata": {
        "id": "ArhhNHd0o4hE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qr6CBuzNkFow"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak\"\n",
        "text= re.compile('[.?!]').split(text)\n",
        "text"
      ],
      "metadata": {
        "id": "0NJ8p6zQpCBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdd30df-878d-423b-daa9-306c423ad65f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION USING NLTK"
      ],
      "metadata": {
        "id": "Rmsw5VUf-tNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "a_LwvSzz_vKh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "QEsewhdQ-yp1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VWWA9vN_oJD",
        "outputId": "493342a8-d330-4bc1-8c7c-91013af3cf12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPSzsb0W-hzG",
        "outputId": "95a1185f-c99a-4df0-b6da-1c57ef5b1ee2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'a',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'within',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'that',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'helping',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'the',\n",
              " 'way',\n",
              " 'that',\n",
              " 'humans',\n",
              " 'write',\n",
              " 'and',\n",
              " 'speak']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmt_Oc4n_dc2",
        "outputId": "e0503b7e-1a19-413a-e360-fe28a7e94edd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION USING KERAS"
      ],
      "metadata": {
        "id": "hfBTuKGiAXXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "HM_AJOx-Ac-n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "text = \"Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak\"\n",
        "text1 = text_to_word_sequence(text)\n",
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DLJtcJBAIX9",
        "outputId": "2df5ac38-2b04-4134-ab5d-f9803e8ced44"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'a',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'within',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'that',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'helping',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'the',\n",
              " 'way',\n",
              " 'that',\n",
              " 'humans',\n",
              " 'write',\n",
              " 'and',\n",
              " 'speak']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION USING GENSIM"
      ],
      "metadata": {
        "id": "SMIqdY4bCU-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "BcnwhWgXCa26"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize"
      ],
      "metadata": {
        "id": "kwsbjhAXCPPs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural language processing  is a branch of artificial intelligence within computer science that focuses on helping computers to understand the way that humans write and speak\"\n",
        "list(tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8LJ6loICkXx",
        "outputId": "0aad0c97-eaf9-4037-823f-edbffc064d5a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'a',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'within',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'that',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'helping',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'the',\n",
              " 'way',\n",
              " 'that',\n",
              " 'humans',\n",
              " 'write',\n",
              " 'and',\n",
              " 'speak']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING"
      ],
      "metadata": {
        "id": "E4Vc1nwqDqsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PORTER STEMMER()"
      ],
      "metadata": {
        "id": "kSmt9w4LD7xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "words = ['Consultant','Consulting','Consultants','consulting']\n",
        "for word in words:\n",
        "    print(word,\"--->\",porter.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_62WtZcECQQ",
        "outputId": "98fc9dac-6b71-4ae0-918c-493d94157a47"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consultant ---> consult\n",
            "Consulting ---> consult\n",
            "Consultants ---> consult\n",
            "consulting ---> consult\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SNOWBALL STEMMER()"
      ],
      "metadata": {
        "id": "Dqfm2SSCEVrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = ['generous','generate','generously','generation']\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LdRvGiyEZt8",
        "outputId": "5078e865-d0c1-483c-af4b-7e14c4cdcb5e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generous ---> generous\n",
            "generate ---> generat\n",
            "generously ---> generous\n",
            "generation ---> generat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LANCASTER STEMMER()"
      ],
      "metadata": {
        "id": "ZzFszH_4Eif7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "words = ['eating','eats','eaten','puts','putting']\n",
        "for word in words:\n",
        "    print(word,\"--->\",lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OClH1OZiDt0w",
        "outputId": "d3d8b51c-3a06-4009-b60f-8d41b404be1d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eats ---> eat\n",
            "eaten ---> eat\n",
            "puts ---> put\n",
            "putting ---> put\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "K3CBNFWeFbRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob,Word"
      ],
      "metadata": {
        "id": "NtxEejf-Cvao"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QzCSdTBDs8Q",
        "outputId": "0110d992-5e50-4070-8576-2df1bda398fd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listofwords = ['troubled' , 'gets' , 'information' , 'changes']"
      ],
      "metadata": {
        "id": "FP1MMuifFiyR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in listofwords:\n",
        "  w = Word(word)\n",
        "  print(word + '--->' + w.lemmatize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S5uZ6xsF3YX",
        "outputId": "88e8b9ad-7a04-4905-bc34-7058393d302a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "troubled--->troubled\n",
            "gets--->get\n",
            "information--->information\n",
            "changes--->change\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "qcrzxn_hF68q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn1=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "iS3EMJ8OF_-P"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for words in listofwords:\n",
        "  print(words + '--->' + wn1.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiR9IAhiGDMS",
        "outputId": "52b474f6-e6ce-4c14-bdba-a18a90605ef3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting--->Connecting\n",
            "Puts--->Puts\n",
            "Generation--->Generation\n",
            "Refers--->Refers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COSINE SIMILARITIES"
      ],
      "metadata": {
        "id": "H5jeEk3mGimp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm"
      ],
      "metadata": {
        "id": "mcOj3M4SGHV-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "FLCPz2itGodC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = nlp(\"I like to open my mind , but my mouth in a general discussion.\")\n",
        "sent2 = nlp(\"I like to open my mind, but not my mouth in a general discussion.\")\n",
        "print(sent2.similarity(sent1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BLGTXPHGreK",
        "outputId": "76ae0f4d-e232-4b41-840c-499e5592b0a6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9661457898093567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGT4nk3NHYzN",
        "outputId": "3a4abfb2-e1fc-40ab-bd20-ec5f6a2dccdd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "nFVBUBThHZkz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpxIsvYeHc9g",
        "outputId": "daa3c7c1-15b4-46f1-b448-3f71b2b830e5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens_1= word_tokenize(\"I like to open my mind, but my mouth in a general discussion.\")\n",
        "word_tokens_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzneMx5_HhYy",
        "outputId": "d0c9c240-87c1-4134-ee7a-91ea15ad98c3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'like',\n",
              " 'to',\n",
              " 'open',\n",
              " 'my',\n",
              " 'mind',\n",
              " ',',\n",
              " 'but',\n",
              " 'my',\n",
              " 'mouth',\n",
              " 'in',\n",
              " 'a',\n",
              " 'general',\n",
              " 'discussion',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1=[w for w in word_tokens_1 if w.lower() not in stop_words]\n",
        "new_sent_1 = ' '.join(str(elem) for elem in new_sent_1)\n",
        "print(new_sent_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkmfZsH4ICBP",
        "outputId": "b80e47c6-df6c-4740-efea-e25e8beb3c91"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "like open mind , mouth general discussion .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens_2= word_tokenize(\"I like to open my mind, but not my mouth in a general discussion.\")\n",
        "new_sent_2=[w for w in word_tokens_1 if w.lower() not in stop_words]\n",
        "new_sent_2 = ' '.join(str(elem) for elem in new_sent_2)\n",
        "print(new_sent_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQBAiF6HIcCq",
        "outputId": "62351e96-b4fa-4a10-e07c-dd76919c5e1c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "like open mind , mouth general discussion .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1 = nlp(new_sent_1)\n",
        "new_sent_2 = nlp(new_sent_2)\n",
        "print(new_sent_2.similarity(new_sent_1))\n"
      ],
      "metadata": {
        "id": "DnQ06GD9IluV",
        "outputId": "b2493d0c-fe3b-4617-c76b-7161702665b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    }
  ]
}